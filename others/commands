mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


cat > /etc/modules-load.d/k8s.conf <<EOF
br_netfilter
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
ip_vs
EOF



global
user haproxy
group haproxy

defaults
mode http
log global
retries 2
timeout connect 3000ms
timeout server 5000ms
timeout client 5000ms

frontend kubernetes
bind 10.128.0.13:6443
option tcplog
mode tcp
default_backend kubernetes-master-nodes

backend kubernetes-master-nodes
mode tcp
balance roundrobin
option tcp-check
server k8s-master-0 10.128.0.14:6443 check fall 3 rise 2
server k8s-master-1 10.128.0.11:6443 check fall 3 rise 2
server k8s-master-2 10.128.0.12:6443 check fall 3 rise 2


cat > /root/kubeadm-config.yml <<EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "10.128.0.13:6443"
networking:
  podSubnet: "10.244.0.0/16"
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "10.128.0.39"
  bindPort: 6443
EOF


kubeadm init --config '/root/kubeadm-config.yml' --upload-certs


Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 10.128.0.13:6443 --token 2wkrhv.pcdpe7qoc3z9e3j2 \
    --discovery-token-ca-cert-hash sha256:40a70e4054a6670bfc22bc7b975e6ec1a0e0a9b749c8d14a5df251f18f30509b \
    --control-plane --certificate-key b94cd6eb79955a7fde00a4d91fab2a54f8bddee8dbcad14cd0b60ceb8bb292d6

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.128.0.13:6443 --token 2wkrhv.pcdpe7qoc3z9e3j2 \
    --discovery-token-ca-cert-hash sha256:40a70e4054a6670bfc22bc7b975e6ec1a0e0a9b749c8d14a5df251f18f30509b

bash <<EOF
# Lembre-se de executar nos outros masters!
apt-get update
apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common vim
curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
echo 'deb https://apt.kubernetes.io/ kubernetes-xenial main' > /etc/apt/sources.list.d/kubernetes.list
apt-get update
apt-get install -y docker-ce docker-ce-cli containerd.io kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl
EOF


cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "journald"
}
EOF

echo "KUBELET_EXTRA_ARGS='--node-ip=10.128.0.18'" > /etc/default/kubelet
echo "KUBELET_EXTRA_ARGS='--node-ip=10.150.0.3'" > /etc/default/kubelet
echo "KUBELET_EXTRA_ARGS='--node-ip=10.142.0.3'" > /etc/default/kubelet


node-role.kubernetes.io/master: ""
key: node-role.kubernetes.io/master


blackpanthersud.us-central1-a.c.getupcloud-266101.internal

EXECUTAR EM TODOS OS MASTERS
cd /opt
wget https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz
tar -zxvf helm-v2.16.1-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/helm


APENAS EM UM MASTER
kubectl -n kube-system create serviceaccount tiller

kubectl create clusterrolebinding tiller \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:tiller

helm init --service-account tiller

result
Creating /root/.helm 
Creating /root/.helm/repository 
Creating /root/.helm/repository/cache 
Creating /root/.helm/repository/local 
Creating /root/.helm/plugins 
Creating /root/.helm/starters 
Creating /root/.helm/cache/archive 
Creating /root/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /root/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation



kubectl -n kube-system  rollout status deploy/tiller-deploy

helm version --short
Client: v2.16.1+gbbdfe5e
Server: v2.16.1+gbbdfe5e


kubectl create ns observability
helm install --name prometheus --namespace observability --set alertmanager.persistentVolume.enabled=false,server.persistentVolume.enabled=false,alertmanager.service.type=NodePort,server.service.type=NodePort stable/prometheus

helm install --name grafana --namespace observability --set service.type=NodePort stable/grafana

kubectl get secret --namespace observability grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
usuario admin
senha yXRXQ3RFb143zGf7bq4N2v6phlfI2rUKjsKo2X3V

helm repo add elastic https://helm.elastic.co

helm install --name elasticsearch --namespace observability --set persistence.enabled=false elastic/elasticsearch

helm install --name kibana --namespace observability --set service.type=NodePort elastic/kibana

helm install --name metricbeat --namespace observability elastic/metricbeat

curl 10.96.65.203:9200/_cat/indices

root@jerry:~# curl 10.96.65.203:9200/_cat/indices
green open .kibana_task_manager_1             pT385wScSPy2Lj2f_NqBQg 1 1    2 6 85.8kb 42.8kb
green open .apm-agent-configuration           Z7cauCFnTO6Jmc8NfP-OeQ 1 1    0 0   460b   230b
green open metricbeat-7.5.2-2020.01.28-000001 XWdEBK2kQheNFhvSbBISxg 1 1 1634 0  3.7mb  1.6mb
green open .kibana_1                          4qPaLHhdRnCrNtGU_l0qEQ 1 1    7 1 58.3kb 29.1kb

https://logz.io/blog/deploying-the-elk-stack-on-kubernetes-with-helm/

cat > /opt/voting-app/k8s-rbac-dev/serviceaccount-dev.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: readonlyuser
  namespace: default
EOF

cat > /opt/voting-app/k8s-rbac-dev/clusterrole-dev.yaml <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: readonlyuser
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - services
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
EOF

cat > /opt/voting-app/k8s-rbac-dev/clusterrolebinding-dev.yaml <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: readonlyuser
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: readonlyuser
subjects:
- kind: ServiceAccount
  name: readonlyuser
  namespace: default
EOF



kubectl create serviceaccount readonlyuser

kubectl create clusterrole readonlyuser --verb=get --verb=list --verb=watch --resource=pods --resource=daemonset --resource=services --resource=ingresses --resource=nodes

kubectl create clusterrolebinding readonlyuser --serviceaccount=default:readonlyuser --clusterrole=readonlyuser

TOKEN=$(kubectl describe secrets "$(kubectl describe serviceaccount readonlyuser | grep -i Tokens | awk '{print $2}')" | grep token: | awk '{print $2}')


kubectl config set-credentials desenvolvedor --token=$TOKEN

kubectl config set-context devreader --cluster=kubernetes --user=desenvolvedor